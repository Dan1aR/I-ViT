{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DDaAex5Q7u-"
      },
      "source": [
        "##### Copyright 2024 The AI Edge Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "W1dWWdNHQ9L0"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y8E0lw5eYWm"
      },
      "source": [
        "# Post-training integer quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIGrZZPTZVeO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_integer_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTC1rDAuei_1"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Integer quantization is an optimization strategy that converts 32-bit floating-point numbers (such as weights and activation outputs) to the nearest 8-bit fixed-point numbers. This results in a smaller model and increased inferencing speed, which is valuable for low-power devices such as [microcontrollers](https://www.tensorflow.org/lite/microcontrollers). This data format is also required by integer-only accelerators such as the [Edge TPU](https://coral.ai/).\n",
        "\n",
        "In this tutorial, you'll train an MNIST model from scratch, convert it into a Tensorflow Lite file, and quantize it using [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization). Finally, you'll check the accuracy of the converted model and compare it to the original float model.\n",
        "\n",
        "You actually have several options as to how much you want to quantize a model. In this tutorial, you'll perform \"full integer quantization,\" which converts all weights and activation outputs into 8-bit integer data—whereas other strategies may leave some amount of data in floating-point.\n",
        "\n",
        "To learn more about the various quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqqUIZjZjac"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nR5AMEWq0H"
      },
      "source": [
        "In order to quantize both the input and output tensors, we need to use APIs added in TensorFlow 2.3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WsN6s5L1ieNl"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(\"TensorFlow version: \", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XsEP17Zelz9"
      },
      "source": [
        "## Generate a TensorFlow Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NMaNZQCkW9X"
      },
      "source": [
        "We'll build a simple model to classify numbers from the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist).\n",
        "\n",
        "This training won't take long because you're training the model for just a 5 epochs, which trains to about ~98% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMsw_6HujaqM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8590 - loss: 0.5167 - val_accuracy: 0.9670 - val_loss: 0.1220\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1144 - val_accuracy: 0.9754 - val_loss: 0.0817\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0747 - val_accuracy: 0.9784 - val_loss: 0.0663\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0626 - val_accuracy: 0.9785 - val_loss: 0.0692\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0529 - val_accuracy: 0.9811 - val_loss: 0.0597\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x12f5d4a60>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "test_images = test_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                  from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=5,\n",
        "  validation_data=(test_images, test_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuTEoGFYd8aM"
      },
      "source": [
        "## Convert to a TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl8_fzVAZwOh"
      },
      "source": [
        "Now you can convert the trained model to TensorFlow Lite format using the TensorFlow Lite [Converter](https://www.tensorflow.org/lite/models/convert), and apply varying degrees of quantization.\n",
        "\n",
        "Beware that some versions of quantization leave some of the data in float format. So the following sections show each option with increasing amounts of quantization, until we get a model that's entirely int8 or uint8 data. (Notice we duplicate some code in each section so you can see all the quantization steps for each option.)\n",
        "\n",
        "First, here's a converted model with no quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i8B2nDZmAgQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5088807744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088903008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "W0000 00:00:1751139652.707897 9284036 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1751139652.707921 9284036 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-06-28 22:40:52.708076: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur\n",
            "2025-06-28 22:40:52.708288: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-06-28 22:40:52.708292: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur\n",
            "I0000 00:00:1751139652.710088 9284036 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
            "2025-06-28 22:40:52.710386: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-06-28 22:40:52.721978: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpacb2d7ur\n",
            "2025-06-28 22:40:52.725648: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 17573 microseconds.\n",
            "2025-06-28 22:40:52.731818: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BONhYtYocQY"
      },
      "source": [
        "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPYZwgZTwJMT"
      },
      "source": [
        "### Convert using dynamic range quantization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjvq1vpJd4U_"
      },
      "source": [
        "Now let's enable the default `optimizations` flag to quantize all fixed parameters (such as weights):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEZ6ET1AHAS3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5088807744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088903008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1751139654.208845 9284036 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1751139654.208900 9284036 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-06-28 22:40:54.209105: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n\n",
            "2025-06-28 22:40:54.209383: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-06-28 22:40:54.209389: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n\n",
            "2025-06-28 22:40:54.211514: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-06-28 22:40:54.224648: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpm0928q8n\n",
            "2025-06-28 22:40:54.228688: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 19579 microseconds.\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5wuE-RcdX_3"
      },
      "source": [
        "The model is now a bit smaller with quantized weights, but other variable data is still in float format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgKDdnHQEhpb"
      },
      "source": [
        "### Convert using float fallback quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTe8avZJHMDO"
      },
      "source": [
        "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a [`RepresentativeDataset`](https://www.tensorflow.org/api_docs/python/tf/lite/RepresentativeDataset). This is a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.)\n",
        "To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiwiWU3gHdkW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5088807744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088903008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1751139656.522954 9284036 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1751139656.522967 9284036 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-06-28 22:40:56.523104: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj\n",
            "2025-06-28 22:40:56.523354: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-06-28 22:40:56.523359: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj\n",
            "2025-06-28 22:40:56.525442: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-06-28 22:40:56.537081: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmpj37sfigj\n",
            "2025-06-28 22:40:56.540801: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 17697 microseconds.\n",
            "2025-06-28 22:40:56.932030: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
          ]
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GC3HFlptf7x"
      },
      "source": [
        "Now all weights and variable data are quantized, and the model is significantly smaller compared to the original TensorFlow Lite model.\n",
        "\n",
        "However, to maintain compatibility with applications that traditionally use float model input and output tensors, the TensorFlow Lite Converter leaves the model input and output tensors in float:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id1OEKFELQwp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.float32'>\n",
            "output:  <class 'numpy.float32'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RACBJuj2XO8x"
      },
      "source": [
        "That's usually good for compatibility, but it won't be compatible with devices that perform only integer-based operations, such as the Edge TPU.\n",
        "\n",
        "Additionally, the above process may leave an operation in float format if TensorFlow Lite doesn't include a quantized implementation for that operation. This strategy allows conversion to complete so you have a smaller and more efficient model, but again, it won't be compatible with integer-only hardware. (All ops in this MNIST model have a quantized implementation.)\n",
        "\n",
        "So to ensure an end-to-end integer-only model, you need a couple more parameters..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQgTqbvPvxGJ"
      },
      "source": [
        "### Convert using integer-only quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwR9keYAwArA"
      },
      "source": [
        "To quantize the input and output tensors, and make the converter throw an error if it encounters an operation it cannot quantize, convert the model again with some additional parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzjEjcDs3BHa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  5088807744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088903008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5088902656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1751139661.447870 9284036 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1751139661.447887 9284036 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
            "2025-06-28 22:41:01.448141: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz\n",
            "2025-06-28 22:41:01.448599: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
            "2025-06-28 22:41:01.448612: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz\n",
            "2025-06-28 22:41:01.453804: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
            "2025-06-28 22:41:01.476071: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/zj/1z0d4jld0bngbqf359gjp7jc0000gn/T/tmptvnxesoz\n",
            "2025-06-28 22:41:01.481046: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 32907 microseconds.\n",
            "2025-06-28 22:41:01.572357: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
          ]
        }
      ],
      "source": [
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYd6NxD03yjB"
      },
      "source": [
        "The internal quantization remains the same as above, but you can see the input and output tensors are now integer format:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaNkOS-twz4k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input:  <class 'numpy.uint8'>\n",
            "output:  <class 'numpy.uint8'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO17AP84wzBb"
      },
      "source": [
        "Now you have an integer quantized model that uses integer data for the model's input and output tensors, so it's compatible with integer-only hardware such as the [Edge TPU](https://coral.ai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sse224YJ4KMm"
      },
      "source": [
        "### Save the models as files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_9nZ4nv4b9P"
      },
      "source": [
        "You'll need a `.tflite` file to deploy your model on other devices. So let's save the converted models to files and then load them when we run inferences below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEY59dC14uRv"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24776"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.compiler.mlir.lite.python import tflite_to_mlir\n",
        "\n",
        "# Загружаем .tflite модель\n",
        "with open(tflite_model_quant_file, 'rb') as f:\n",
        "    tflite_model = f.read()\n",
        "\n",
        "# Конвертируем в MLIR\n",
        "mlir_output = tflite_to_mlir.tflite_to_mlir_text(tflite_model)\n",
        "\n",
        "# Сохраняем результат\n",
        "with open('mnist_model.mlir', 'w') as f:\n",
        "    f.write(mlir_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t9yaTeF9fyM"
      },
      "source": [
        "## Run the TensorFlow Lite models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8lQHMp_asCq"
      },
      "source": [
        "Now we'll run inferences using the TensorFlow Lite [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to compare the model accuracies.\n",
        "\n",
        "First, we need a function that runs inference with a given model and images, and then returns the predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X092SbeWfd1A"
      },
      "outputs": [],
      "source": [
        "# Helper function to run inference on a TFLite model\n",
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_images\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_images[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    print(f\"{output=}; {output.dtype=}\")\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opUt_JTdyEu"
      },
      "source": [
        "### Test the models on one image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPpFPaz7eEM"
      },
      "source": [
        "Now we'll compare the performance of the float model and quantized model:\n",
        "+ `tflite_model_file` is the original TensorFlow Lite model with floating-point data.\n",
        "+ `tflite_model_quant_file` is the last model we converted using integer-only quantization (it uses uint8 data for input and output).\n",
        "\n",
        "Let's create another function to print our predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR2cHRUcUZ6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Change this to test a different image\n",
        "test_image_index = 1\n",
        "\n",
        "## Helper function to test the models on one image\n",
        "def test_model(tflite_file, test_image_index, model_type):\n",
        "  global test_labels\n",
        "\n",
        "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
        "\n",
        "  plt.imshow(test_images[test_image_index])\n",
        "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
        "  _ = plt.title(template.format(true= str(test_labels[test_image_index]), predict=str(predictions[0])))\n",
        "  plt.grid(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5OTJ_6Vcslt"
      },
      "source": [
        "Now test the float model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTK0x980coto"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output=array([ -4.9357505,  -5.7554646,   8.630977 ,  -7.7143235, -16.521345 ,\n",
            "        -9.395366 ,  -2.8384545, -20.91402  ,  -5.0032244, -18.01632  ],\n",
            "      dtype=float32); output.dtype=dtype('float32')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danil-andreev/Projects/I-ViT/.venv39/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3N6-UGl1dfE"
      },
      "source": [
        "And test the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1i9umMcp0t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output=array([161, 157, 238, 145,  96, 137, 173,  71, 160,  87], dtype=uint8); output.dtype=dtype('uint8')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAHICAYAAAAIkT5uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqQklEQVR4nO3dCXxU1d3/8V/YwiIEIUASCUhYlU1FREQ2oURUdlFELVgKBUHZFIu1IupTVFqlKmL1r0Qri2AFlEdRZBUbVFCKKFKCrAJhKSEQJEC4z+t3+M90JpkE7pDkTGY+79frOszMvTNnbsb7nXPuuedEOY7jCAAAxaxUcb8hAACKAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIuQKdOncxSnFauXClRUVHmNlRp+Z544gnX2+3YscNsm5KSUiTlQslAAKHIff/993LPPffIZZddJtHR0ZKQkGDu//DDDxJKtDx6MNWDY0miB3E9mOuyZs2aPM/raFuJiYnm+dtuu81KGYFACCAUqffff1+uueYaWbZsmdx3333yyiuvyJAhQ2T58uXm8UWLFkkoBdDkyZMDBtCnn35qllBWvnx5mT17dp7HV61aJXv27DHhD4SSMrYLgPC1bds2uffeeyUpKUlWr14tNWrU8D43evRoad++vakJbdy4UerVqyehrFy5chLqbrnlFpk/f768+OKLUqbMf//X1lBq1aqVHDp0yGr5gNyoAaHITJ06VU6cOCGvvfaaX/io2NhY+dvf/ibHjx8363kMHjxYLr/88jyvpU1j2oTka+bMmXLTTTdJzZo1za/7K6+8UmbMmJFnW309bXrS5qnrrrvO1BQ0FN9++22/Zqz+/fubf3fu3NnbpOU5/5L7HJC+pmed3IvvOZuff/5ZfvOb30itWrVMGZs2bSpvvvlmnjJqDaV3795SqVIl83nGjh0r2dnZ4sZdd90lhw8flqVLl3ofO3XqlLz33nsycODAgNtkZWXJ+PHjTROdlq9x48by5z//2TTb+dKyaJn071i5cmXp2bOnKXMgF/qZAWpAKDIffvihOVBrTSeQDh06mOd1PW2ac0vDRg9uejDUX/z6Ovfff7+cPXtWRo4c6bduWlqa3H777ab5b9CgQeaAqGGnNQN9DS3Lgw8+aGoPjz76qFxxxRVmO89tbtOmTTPh6euFF16QDRs2SPXq1c399PR0uf76600ojRo1yhy8P/74Y1OGzMxMGTNmjFnvl19+kS5dusiuXbtMGfQc2d///nfTTOmG7su2bdvKnDlzpHv37uYxfb+jR4/KgAEDzGfzpSGj+27FihWmTFdddZV88skn8vDDD5sQ0c/j8dvf/lbeeecdE2Q33HCDKdutt96apwwX+pkBQ+cDAgpbRkaG/oR2evXqVeB6PXv2NOtlZmaa+4MGDXLq1q2bZ71JkyaZ9XydOHEiz3rJyclOUlKS32P6errt6tWrvY8dOHDAiY6OdsaPH+99bP78+Wa9FStW5Hndjh07miU/8+bNM9s++eST3seGDBnixMfHO4cOHfJbd8CAAU5MTIy3/NOmTTPb6mt4ZGVlOQ0aNMi3PL5mzpxp1vv666+dl19+2alcubL3tfv37+907tzZux9uvfVW73YLFy402z399NN+r3f77bc7UVFRTlpamrm/YcMGs97999/vt97AgQPN4/q3cfuZt2/fbrbVsiNy0QSHInHs2DFzq801BfE871nfjQoVKnj/rb/y9RxHx44d5aeffjL3fWnznG9NTH+Za3OTrlsYnRe0yalXr17y2GOPeWsX//jHP6RHjx7m31o2z5KcnGzK980335h1P/roI4mPjzc1NI+KFSvKsGHDXJfljjvuMDWqxYsXm32qt/k1v+n7li5d2tS6fGmTnJZZay6e9VTu9XLXZtx8ZkDRBIcicaHBos9rc42eE3Lriy++kEmTJklqaqo51+RLD3YxMTHe+3Xq1Mmz/aWXXipHjhyRi6HNSn379jVdzPWckuc81cGDByUjI8Oc/9IlkAMHDpjbnTt3SoMGDfKc49KAdEuDtWvXrqbjge6TnJwcv2Dzpe+rzX25fyR4mh31ec9tqVKlpH79+gWWz81nBhQBhCKhB389uGkPt4Lo87Vr1/b2Mst9EPbQA2nuHnZ63qRJkyby/PPPm5Po+hr6a13PXeh5IF/6Sz+Qi52RXs8j7d27V7766iupUqWK93HP+2svPz3nFEiLFi2kKGiNZ+jQobJ//35zLqhq1apSHGx+ZpRMBBCKjDbFaE837X1244035nn+888/N9fcjBs3zq9Wor+ic/P8GvfQDgfaM+uDDz7wq93oCfVg5Rd++XnmmWdk4cKF5lonDUJfnt5iGpxaIylI3bp1ZdOmTSYMfcuwZcsWCUafPn3kd7/7naxdu1befffdAt/3s88+M7VQ31rQjz/+6H3ec6vhoqHvW+vJXT43nxlQnANCkXnooYfMuQw9GGr3YF//+c9/ZPjw4abWoL2lPLSZR5vPfGtO+/btkwULFgSs0fjWYHQ77ZodLO0CrQIFYG564NbzPX/4wx9M9+nctHz9+vUz50Q0XHLT5irf63e0FqXdpT083deDcckll5gegtp1XX8E5EffV8Pi5Zdf9ntca5AahJ6edJ7b3L3otCdgsJ8ZUNSAUGT0vIaeF9HrU5o3b2664uoFp1rreeONN8z5l7lz5/pdhKrdhR955BHzK15PeuuBWA+mjRo18juB3a1bN9PkpgdYDTjtEv3666+ba2g0sIKh3ZD1IPrss8+aMNNrWDzXGeWmn0l/8Tds2NB0T/b1q1/9ylwDozUkrZG1adPGNIlpRwgNXv0cGmD6b6XPaQj8+te/lvXr15sOCdoNW8M7WPk1gfnSfafXPGmI6t+kZcuWZrQHHZ1COxh4zvnoftHPq13ldb9oN2wd2UK7tud2oZ8ZMGx3w0P4++6770yX3bi4OKdUqVKm+2358uWd77//PuD6n376qdOsWTOnXLlyTuPGjZ133nknYDfsDz74wGnRooV5rcsvv9x59tlnnTfffNOsp918PXJ3Py6oa/Xrr79uunGXLl3arwt07nX1ufwW327T6enpzsiRI53ExESnbNmyZh906dLFee211/zed+fOnaZLesWKFZ3Y2Fhn9OjRzpIlS1x3wy5IoP1w7NgxZ+zYsU5CQoIpX8OGDZ2pU6c6Z8+e9Vvvl19+cR588EGnevXqTqVKlZwePXo4u3fvztMN+0I/M92woaL0P2QxipPWivTkvZ6s9h2NAEBkoQkOxU6bmrSZ7Pe//73pAfenP/3JdpEAWEANCABgBb3gAABWEEAAACsIIACAFQQQAMAKAggIAzoJXu7J8PKb3C+UyojIRgChUOhsofnNEOq76PAwNumQQDoDq05ApyMZ6ECdOoFaQWOmBfP5q1WrJq1btzYT3+UeGDXUabd4HePOFh39Yvr06Wa0Cx0VQseXu/rqq82IGLkHpUXJxnVAKBQ6nIvOmunx9ddf55ldNBRGQ9apG7SsOg6ajuWmM6nq2GU6BJDO6zN58uSgX1uvaZoyZYp33DO9yFaHH/r3v/9thqgpbjo0UTDhpwGkUzgEGuOuOOgcTQ888IAZ7VwHqtXxAnWmVp3tVgdYfeutt6yUC0WAASFQFAqaXdTX8ePHneL0008/OTt27PB7TIeduemmm8wMqcGWR4fpadq0qd9jOqtp7dq1zdA1p06dCrhdTk6OGebmYul+vpD9fSG0vDozbWG70DIePHjQ2bRpU57H77vvPrP91q1bC71ssIMmOBQbbX7T5imtaeicNTr1gmeaBm3C0iW3QOcx9Fe9jsTctGlTKV++vBn4UwckzT25nA6cqVML+M6OqgOfeqYZ8NAy6a99nd6hMGZI9dDBRLV5LysryzsStL6Xjv49a9YsU34d8HTJkiXmuZ9//tnMrKqfRx/X57UJL7c9e/aY8uro3TpQ6tixY03ZL3Tf/fWvfzWDw+q+02bIm2++WdatW+ctn5ZXaxme5kR9HY/CLqM2t+nfSGdN9dDJCfV1c9MBatXmzZvPs+dRUtAEh2LXv39/M4q0NvUEMxCHhk1KSorcd999ZsTs7du3m9Gkv/32WzNLatmyZc16OoWDrqNTNPgeRAPRydtUMDOzFkQDTUfY9p0Ubvny5TJv3jwTRPp+GhLp6ekmrDwBpcGgU2JrE57OuuqZ/lqn29amqV27dpnPrpP+6cjZ+poXQl9P951OsaBNpmfOnDHzMmnT1rXXXmteSx+/7rrrvFOCe0bFLooy6kR+OiK3zmx7vvODRfU3gkWWal6IwCY4z4jWd9111wWNTK20KUhHcfb4/PPPzWvMmjXLbz3PyNG+j3tGiT7fiMuHDx92atas6bRv39715/Qtf5MmTUzzkS6bN282o0fr++vI0R56X0cEzz0S+JAhQ5z4+Hjn0KFDfo8PGDDAiYmJcU6cOGHuT5s2zbzGvHnz/Jr6GjRokGd/5953y5cvN+touXLzHf06vya4oiijp1ku94jauWVnZztXXnmlU69ePef06dMFrouSgyY4FDudiC5Y8+fPN9N965w72mzjWVq1amUmYvOdEVVrPXrML6j2o01Sd999t5mE7qWXXpKLoU1JWivQRTte6OvdeuuteZqoOnbsaObJ8dAyakcInZ9H/+37uZKTk00TomcuJJ1yXHuGaScB36Y+T22lIPoeWnvR2obb2WCLqoza7Kqvd77aj9a4tOlWa7racQThgb8kip3vBHRubd261RzsAk0Spw4cOODq9bS3lZ6D0R5rOiHbxdCmNO15pgdzPb+izYyBypn78+v5IQ1AnQE1v1lQPZ9LpybXif5yB4bvVNn50Sm1tTlMu4i7VVxlDES7zet+feqpp0zvRYQPAgjFrkKFCnke04NVoPNBua/70BqLHtT1JH4gWvu4UNrlWmf51C7S9957r1wsPeHetWtX15/f01Va50fKbyZT293XbZVRz1fpDLlaa9Zu8wgvBBBCgvaIC9QDTX9N+9IT4jq1c7t27QIG2YXSCx212UdPnOsBziYNTb3YUsP2fAGmPfg2bdpkwtq3hrFly5bzvo/uO72eRqfFLqgWFKg5rrjK6EunBtcOEX379jV/L4QfzgEhJOjBUc+heLorq3/961+mV5uvO+64wxwEtTkmN+3Rpc1EBXXDVjrqgfbO0nM/zz//vNimveT69etnzrHogTs3332iTVB79+6V9957z68rc37NYr70PTQUAl1s61v71Jqc734syjIG6oatVq9ebS4O1hErtLZbqhSHqnBEDQghQa8t0TDQE9rarVfPJ7z66qvmehDt4ut7Al+7YeuIAxs2bDDDtWi3az03pB0U9BoXz8nvQN2wtduvzshavXp101U4d1PeDTfcIElJSd77+gte37Ooxy/TZkDtQNGmTRsZOnSo6aSgNRU9sa81Pv230uf0RLx+hvXr15uT/drFWU/yn492d9amRh2hQveXXv+jTWvaDVuf0xP9Sjt06Hvq30PPGek5Ky1XUZQxUDdsrfX27NnT7Hv9W+rfNXdTn+0mSRQS293wEHndsLWbciDvvPOOk5SU5JQrV8656qqrnE8++SRPV2KP1157zWnVqpVToUIFp3Llyk7z5s2dCRMmOHv37i2wG7bnsfwW33WPHTtmHtNuxsGMhBCIvt7IkSMDPpeenm6eS0xMdMqWLevExcU5Xbp0MZ/V186dO52ePXs6FStWdGJjY53Ro0d7u6EX1A1bnTlzxpk6darpMq77uUaNGk737t2d9evXe9f58ccfnQ4dOph9q6/p2yW7sMsYqBu257H8lvN12UbJwZTcQD60O/Ftt91mmgJ15AAAhYuGVSAf2tyk5yEIH6BoUAMCAFhBDQgAYAUBBACwggACAFhBAAEArAi5C1H1wji9ilqH/TjfCL0AgNCjfduOHTtmLmQuaBSLkAsgDZ/ExETbxQAAXKTdu3dL7dq1S04Aac1H3Si3SBk5N7MlAKDkOCOnZY185D2eF3sA6ei1Oo+HTqOr86zo5Fw6ze/5eJrdNHzKRBFAAFDi/P+rS893GqVIOiHoaMPjxo0zAwzqQIUaQDrIpNvJwgAA4atIAkhH0dURcXUkYh0xV0c11pFwc09NDACIXIUeQKdOnTJDsPtOWqW9IPR+ampqnvWzs7PNcPu+CwAg/BV6AOnEUjphWK1atfwe1/t6Pig3ndclJibGu9ADDgAig/ULUSdOnGhmrPQs2m0PABD+Cr0XXGxsrJm+Nz093e9xvR8XF5dn/ejoaLMAACJLodeAypUrZ6b0XbZsmd/oBnq/bdu2hf12AIASqkiuA9Iu2IMGDZJrr73WXPszbdo0ycrKMr3iAAAosgC688475eDBg/L444+bjgdXXXWVLFmyJE/HBABA5Aq5GVG1G7b2huskvRgJAQBKoDPOaVkpi0zHsipVqoRuLzgAQGQigAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVpSx87bAhdvxdFvX2+SUd4J6rxpND7reJrXlP6Q41F9+n+ttKn9VIaj3qvXiP4PaDnCDGhAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWMFgpChWR/63oettNl31soSy08GNe+raj53/n+ttZl0bH9R7zVva0fU2OZu3BvVeiFzUgAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgYjRbEOLPrFVXMllL2akeR6m+dTf+V6m8vrHnS9zadXvu96m7sr75Ng/M/gWNfbJD3CYKRwhxoQAMAKAggAEB4B9MQTT0hUVJTf0qRJk8J+GwBACVck54CaNm0qn3322X/fpAynmgAA/ookGTRw4uLiiuKlAQBhokjOAW3dulUSEhIkKSlJ7r77btm1a1e+62ZnZ0tmZqbfAgAIf4UeQG3atJGUlBRZsmSJzJgxQ7Zv3y7t27eXY8eOBVx/ypQpEhMT410SExMLu0gAgEgIoO7du0v//v2lRYsWkpycLB999JFkZGTIvHnzAq4/ceJEOXr0qHfZvXt3YRcJABCCirx3QNWqVaVRo0aSlpYW8Pno6GizAAAiS5FfB3T8+HHZtm2bxMfHF/VbAQAiOYAeeughWbVqlezYsUP++c9/Sp8+faR06dJy1113FfZbAQBKsEJvgtuzZ48Jm8OHD0uNGjXkxhtvlLVr15p/AwBQZAE0d25oDzaJvM50aRXUdstbTg9iq7Kut5h2pJHrbVbcea0EZe8B15s0OrLO9Talypd3vc2fvmzueptHY7+TYJy59ExQ2wFuMBYcAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAITnhHQIfccvKxfUdqWC+P0SzMCiK3u6H4Qz56ctEsrSJl/tepvZ1f4SxDsFN9lj7SX8NkXR41sGALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKxgNG1L17dSgtrt93T2ut4k6kul6mzP7dki4+e0tn7ne5pJSwY1sDYQqakAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAWDkSJoOT/823YRQsKO/2nrepshVf8cxDuVd73F+H3XB/E+IpU/2+x6m5yg3gmRjBoQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjBYKSAj4x73Q8s+sWv3Q8sGlPK/cCiqdmlXW+z4emrJRgVMr8KajvADWpAAAArCCAAQMkIoNWrV0uPHj0kISFBoqKiZOHChX7PO44jjz/+uMTHx0uFChWka9eusnXr1sIsMwAgEgMoKytLWrZsKdOnTw/4/HPPPScvvviivPrqq/Lll19KpUqVJDk5WU6ePFkY5QUARGonhO7du5slEK39TJs2TR577DHp1auXeeztt9+WWrVqmZrSgAEDLr7EAICwUKjngLZv3y779+83zW4eMTEx0qZNG0lNTQ24TXZ2tmRmZvotAIDwV6gBpOGjtMbjS+97nsttypQpJqQ8S2JiYmEWCQAQoqz3gps4caIcPXrUu+zevdt2kQAAJS2A4uLizG16errf43rf81xu0dHRUqVKFb8FABD+CjWA6tWrZ4Jm2bJl3sf0nI72hmvb1v0V5gCA8OW6F9zx48clLS3Nr+PBhg0bpFq1alKnTh0ZM2aMPP3009KwYUMTSH/84x/NNUO9e/cu7LIDACIpgNatWyedO3f23h83bpy5HTRokKSkpMiECRPMtULDhg2TjIwMufHGG2XJkiVSvrz7sa8AAOErytGLd0KINtlpb7hO0kvKRJW1XRxEmLQXrne9zY93BL4ou7A1+uR37rf5zboiKQtQkDPOaVkpi0zHsoLO61vvBQcAiEwEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgCUjOkYgJLg1NK6QW2X2uQvQWzlfqqRlqmDXG9zxfhtrrfJcb0FUHyoAQEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQxGipBXJuly19s81WB+UO91aSn3A4uuz3b/PnWfcj9MaM6RI+7fCAhh1IAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoGI0XIqz/vZ9fbXF2u+H5b3bVsuOttGv3r6yIpC1CSUAMCAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsYjBTF6sigtq63mVzrL0G8U3QQ24gM2tHV9TZXTEhzvU2O6y2A8EMNCABgBQEEACgZAbR69Wrp0aOHJCQkSFRUlCxcuNDv+cGDB5vHfZebb765MMsMAIjEAMrKypKWLVvK9OnT811HA2ffvn3eZc6cORdbTgBApHdC6N69u1kKEh0dLXFxcRdTLgBAmCuSc0ArV66UmjVrSuPGjWXEiBFy+PDhfNfNzs6WzMxMvwUAEP4KPYC0+e3tt9+WZcuWybPPPiurVq0yNaacnMAdT6dMmSIxMTHeJTExsbCLBACIhOuABgwY4P138+bNpUWLFlK/fn1TK+rSpUue9SdOnCjjxo3z3tcaECEEAOGvyLthJyUlSWxsrKSlpeV7vqhKlSp+CwAg/BV5AO3Zs8ecA4qPjy/qtwIAhHMT3PHjx/1qM9u3b5cNGzZItWrVzDJ58mTp16+f6QW3bds2mTBhgjRo0ECSk5MLu+wAgEgKoHXr1knnzp299z3nbwYNGiQzZsyQjRs3yltvvSUZGRnmYtVu3brJU089ZZraAAAIOoA6deokjuPk+/wnn3zi9iVRQpW5LMH1Nu0f/NL1NpeUKr4fL6k/NHC9TaMjXxdJWYBwx1hwAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQACI8puRE5Nj/qfur0hXEfSnHo/F3/oLa7YkLgmXsLkhPUOwGgBgQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVjAYKYK2vucLQWwVLcUh5v6zQW135siRQi8LgMCoAQEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQxGirB0ulZMUNuVPXWZhJOcg4eC2s7Jzna9TVS0+4FmS9eIleKQU6NqUNttHV9OQpWTExXUdk0eSHO9TU5mphQFakAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAWDkSIs/e97b9ouQki44du7gtruUHoV19tcWuOY622+bDXb9Ta4OFc+Nsr1NkkTUqUoUAMCAFhBAAEAQj+ApkyZIq1bt5bKlStLzZo1pXfv3rJlyxa/dU6ePCkjR46U6tWryyWXXCL9+vWT9PT0wi43ACCSAmjVqlUmXNauXStLly6V06dPS7du3SQrK8u7ztixY+XDDz+U+fPnm/X37t0rffv2LYqyAwAipRPCkiVL/O6npKSYmtD69eulQ4cOcvToUXnjjTdk9uzZctNNN5l1Zs6cKVdccYUJreuvv75wSw8AiMxzQBo4qlq1auZWg0hrRV27dvWu06RJE6lTp46kpgbuRZGdnS2ZmZl+CwAg/AUdQGfPnpUxY8ZIu3btpFmzZuax/fv3S7ly5aRqVf/512vVqmWey++8UkxMjHdJTEwMtkgAgEgIID0XtGnTJpk7d+5FFWDixImmJuVZdu/efVGvBwAI4wtRR40aJYsXL5bVq1dL7dq1vY/HxcXJqVOnJCMjw68WpL3g9LlAoqOjzQIAiCyuakCO45jwWbBggSxfvlzq1avn93yrVq2kbNmysmzZMu9j2k17165d0rZt28IrNQAgsmpA2uymPdwWLVpkrgXynNfRczcVKlQwt0OGDJFx48aZjglVqlSRBx54wIQPPeAAAEEH0IwZM8xtp06d/B7XrtaDBw82/37hhRekVKlS5gJU7eGWnJwsr7zyipu3AQBEgChH29VCiHbD1ppUJ+klZaLK2i4OCvDLJ/5NsBdiWbP3iqQsiBwnnFOutzntnJXicsvGcz/G3Ti6IVaKS/yaM663if74a1frn3FOy0pZZDqWaUtYfhgLDgBgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAACVnRlRAVUje7nqbpn8a5XobJ8S/pZWb/Mf1Nl+2mi2hrOnn97nextlVSYpD0nvH3W/01XdSXC6VrcWyTTigBgQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVoT4MI8IN/UeTbVdhJBwm7SSUFZPNtouAiIANSAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQACP0AmjJlirRu3VoqV64sNWvWlN69e8uWLVv81unUqZNERUX5LcOHDy/scgMAIimAVq1aJSNHjpS1a9fK0qVL5fTp09KtWzfJysryW2/o0KGyb98+7/Lcc88VdrkBACVcGTcrL1myxO9+SkqKqQmtX79eOnTo4H28YsWKEhcXV3ilBACEnYs6B3T06FFzW61aNb/HZ82aJbGxsdKsWTOZOHGinDhxIt/XyM7OlszMTL8FABD+XNWAfJ09e1bGjBkj7dq1M0HjMXDgQKlbt64kJCTIxo0b5ZFHHjHnid5///18zytNnjw52GIAAEqoKMdxnGA2HDFihHz88ceyZs0aqV27dr7rLV++XLp06SJpaWlSv379gDUgXTy0BpSYmCidpJeUiSobTNEAABadcU7LSllkWsmqVKlSuDWgUaNGyeLFi2X16tUFho9q06aNuc0vgKKjo80CAIgsrgJIK0sPPPCALFiwQFauXCn16tU77zYbNmwwt/Hx8cGXEgAQ2QGkXbBnz54tixYtMtcC7d+/3zweExMjFSpUkG3btpnnb7nlFqlevbo5BzR27FjTQ65FixZF9RkAAOF+DkgvKg1k5syZMnjwYNm9e7fcc889smnTJnNtkJ7L6dOnjzz22GMFtgP60nNAGmicAwKAkqlIzgGdL6s0cPRiVQAAzoex4AAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVpSREOM4jrk9I6dFzv0TAFCCmOO3z/G8xATQsWPHzO0a+ch2UQAAF3k8j4mJyff5KOd8EVXMzp49K3v37pXKlStLVFSU33OZmZmSmJgou3fvlipVqkikYj+cw344h/1wDvshdPaDxoqGT0JCgpQqVark1IC0sLVr1y5wHd2pkfwF82A/nMN+OIf9cA77ITT2Q0E1Hw86IQAArCCAAABWlKgAio6OlkmTJpnbSMZ+OIf9cA774Rz2Q8nbDyHXCQEAEBlKVA0IABA+CCAAgBUEEADACgIIAGBFiQmg6dOny+WXXy7ly5eXNm3ayFdffSWR5oknnjCjQ/guTZo0kXC3evVq6dGjh7mqWj/zwoUL/Z7XfjSPP/64xMfHS4UKFaRr166ydetWibT9MHjw4Dzfj5tvvlnCyZQpU6R169ZmpJSaNWtK7969ZcuWLX7rnDx5UkaOHCnVq1eXSy65RPr16yfp6ekSafuhU6dOeb4Pw4cPl1BSIgLo3XfflXHjxpmuhd988420bNlSkpOT5cCBAxJpmjZtKvv27fMua9askXCXlZVl/ub6IySQ5557Tl588UV59dVX5csvv5RKlSqZ74ceiCJpPygNHN/vx5w5cyScrFq1yoTL2rVrZenSpXL69Gnp1q2b2TceY8eOlQ8//FDmz59v1tehvfr27SuRth/U0KFD/b4P+v9KSHFKgOuuu84ZOXKk935OTo6TkJDgTJkyxYkkkyZNclq2bOlEMv3KLliwwHv/7NmzTlxcnDN16lTvYxkZGU50dLQzZ84cJ1L2gxo0aJDTq1cvJ5IcOHDA7ItVq1Z5//Zly5Z15s+f711n8+bNZp3U1FQnUvaD6tixozN69GgnlIV8DejUqVOyfv1606ziO16c3k9NTZVIo01L2gSTlJQkd999t+zatUsi2fbt22X//v1+3w8dg0qbaSPx+7Fy5UrTJNO4cWMZMWKEHD58WMLZ0aNHzW21atXMrR4rtDbg+33QZuo6deqE9ffhaK794DFr1iyJjY2VZs2aycSJE+XEiRMSSkJuMNLcDh06JDk5OVKrVi2/x/X+jz/+KJFED6opKSnm4KLV6cmTJ0v79u1l06ZNpi04Emn4qEDfD89zkUKb37SpqV69erJt2zZ59NFHpXv37ubAW7p0aQk3OnL+mDFjpF27duYAq/RvXq5cOalatWrEfB/OBtgPauDAgVK3bl3zg3Xjxo3yyCOPmPNE77//voSKkA8g/JceTDxatGhhAkm/YPPmzZMhQ4ZYLRvsGzBggPffzZs3N9+R+vXrm1pRly5dJNzoORD98RUJ50GD2Q/Dhg3z+z5oJx39HuiPE/1ehIKQb4LT6qP+esvdi0Xvx8XFSSTTX3mNGjWStLQ0iVSe7wDfj7y0mVb//wnH78eoUaNk8eLFsmLFCr/pW/Rvrs32GRkZEfF9GJXPfghEf7CqUPo+hHwAaXW6VatWsmzZMr8qp95v27atRLLjx4+bXzP6yyZSaXOTHlh8vx86IZf2hov078eePXvMOaBw+n5o/ws96C5YsECWL19u/v6+9FhRtmxZv++DNjvpudJw+j4459kPgWzYsMHchtT3wSkB5s6da3o1paSkOD/88IMzbNgwp2rVqs7+/fudSDJ+/Hhn5cqVzvbt250vvvjC6dq1qxMbG2t6wISzY8eOOd9++61Z9Cv7/PPPm3/v3LnTPP/MM8+Y78OiRYucjRs3mp5g9erVc3755RcnUvaDPvfQQw+Znl76/fjss8+ca665xmnYsKFz8uRJJ1yMGDHCiYmJMf8f7Nu3z7ucOHHCu87w4cOdOnXqOMuXL3fWrVvntG3b1izhZMR59kNaWprz5JNPms+v3wf9fyMpKcnp0KGDE0pKRACpl156yXypypUrZ7plr1271ok0d955pxMfH2/2wWWXXWbu6xct3K1YscIccHMv2u3Y0xX7j3/8o1OrVi3zQ6VLly7Oli1bnEjaD3rg6datm1OjRg3TDblu3brO0KFDw+5HWqDPr8vMmTO96+gPj/vvv9+59NJLnYoVKzp9+vQxB+dI2g+7du0yYVOtWjXz/0SDBg2chx9+2Dl69KgTSpiOAQBgRcifAwIAhCcCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAiA3/Byxh3AduDDlmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwN7uIdCd8Gw"
      },
      "source": [
        "### Evaluate the models on all images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFKOD4DG8XmU"
      },
      "source": [
        "Now let's run both models using all the test images we loaded at the beginning of this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aeAuWjvjPx"
      },
      "outputs": [],
      "source": [
        "# Helper function to evaluate a TFLite model on all images\n",
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_images\n",
        "  global test_labels\n",
        "\n",
        "  test_image_indices = range(test_images.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_images)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnFilQpBuMh5"
      },
      "source": [
        "Evaluate the float model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5mWkSbMcU5z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model accuracy is 98.1100% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3cY9ry8ZlG"
      },
      "source": [
        "Evaluate the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9cnwiPp6EGm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantized model accuracy is 98.0800% (Number of test samples=10000)\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lfxkor8pgv"
      },
      "source": [
        "So you now have an integer quantized a model with almost no difference in the accuracy, compared to the float model.\n",
        "\n",
        "To learn more about other quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "post_training_integer_quant.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
